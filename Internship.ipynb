{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Internship.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+zf821UjnXlmTQP7HXcYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitgothi/Internship-Project/blob/master/Internship.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cKQXVZslwT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d50c03f-a03a-4a24-9916-1cc7c2ac5da6"
      },
      "source": [
        "import hashlib\n",
        "import os\n",
        "full_path = '/content/aa/AWS1.txt'  # your file\n",
        "os.path.isfile(full_path)\n",
        "print(hashlib.md5(open(full_path, 'rb').read()).hexdigest())\n",
        "#os.listdir(full_path)\n",
        "#[(fname, hashlib.md5(open(fname, 'rb').read()).digest()) for fname in os.listdir(full_path)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59938198f5faaf75d344a8a58abf88f6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VHCLQ8VoYJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60ec07ee-837f-4504-c930-3d9e589361d8"
      },
      "source": [
        "import hashlib\n",
        "import os\n",
        "full_path2 = '/content/aa/AWS2.txt'  # your file\n",
        "os.path.isfile(full_path2)\n",
        "print(hashlib.md5(open(full_path2, 'rb').read()).hexdigest())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59938198f5faaf75d344a8a58abf88f6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNvOTokqqNGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "910f901c-4682-41b7-bd41-e672c1110b1b"
      },
      "source": [
        "import hashlib\n",
        "import os\n",
        "full_path3 = '/content/aa/AWS3.txt'  # your file\n",
        "os.path.isfile(full_path)\n",
        "print(hashlib.md5(open(full_path3, 'rb').read()).hexdigest())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f12adae75020a0820dba6ad1002c966d\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRBWZFvJsfuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hashlib\n",
        "import sys\n",
        "import os\n",
        "\n",
        "path = input(\"Enter Path: \")\n",
        "\n",
        "if not os.path.isdir(path):\n",
        "    print(\"Oops!! Directory not found!\")\n",
        "else:\n",
        "    file_path = []\n",
        "    for root, dir, files in os.walk(path):\n",
        "        for f in files:\n",
        "            filepath = os.path.join(root, f)\n",
        "            file_path.append(filepath)\n",
        "\n",
        "    file_size = {}\n",
        "    for filenames in file_path:\n",
        "        size = os.path.getsize(filenames)\n",
        "\n",
        "        if size in file_size.keys():\n",
        "            file_size[size].append(filenames)\n",
        "        else:\n",
        "            file_size[size] = []\n",
        "            file_size[size].append(filenames)\n",
        "            \n",
        "    same_file_size = {}\n",
        "    for size in file_size:\n",
        "        x = file_size[size]\n",
        "        if (len(x) > 1):\n",
        "            same_file_size[size] = x\n",
        "\n",
        "    def half_md5(x):\n",
        "        md5Hash = hashlib.md5()\n",
        "        with open(x, 'rb') as f:\n",
        "            chunk_size = 1024\n",
        "            data = f.read(chunk_size)\n",
        "            md5Hash.update(data)\n",
        "            return md5Hash.hexdigest()\n",
        "\n",
        "    hash_list = {}\n",
        "    for size in same_file_size:\n",
        "        path = same_file_size[size]\n",
        "        for i in path:\n",
        "            l = half_md5(i)\n",
        "            if l in hash_list.keys():\n",
        "                hash_list[l].append(i)\n",
        "            else:\n",
        "                hash_list[l] = []\n",
        "                hash_list[l].append(i)\n",
        "\n",
        "    empty = bool(hash_list)\n",
        "\n",
        "    if empty is True:\n",
        "\n",
        "        duplicate_files = {}\n",
        "        for y in hash_list:\n",
        "            File = hash_list[y]\n",
        "            if (len(File) > 1):\n",
        "                duplicate_files[y] = File\n",
        "\n",
        "        def hash_file(filename):\n",
        "            h = hashlib.md5()\n",
        "            with open(filename,'rb') as file:\n",
        "                chunk = 0\n",
        "                while chunk != b'':\n",
        "                    chunk = file.read(1024)\n",
        "                    h.update(chunk)\n",
        "            return h.hexdigest()\n",
        "        \n",
        "        full_file_hash = {}\n",
        "        for i in duplicate_files:\n",
        "            file_paths = duplicate_files[i]\n",
        "            for k in file_paths:\n",
        "                full_md5 = hash_file(k)\n",
        "                if full_md5 in full_file_hash.keys():\n",
        "                    full_file_hash[full_md5].append(k)\n",
        "                else:\n",
        "                    full_file_hash[full_md5] = []\n",
        "                    full_file_hash[full_md5].append(k)\n",
        "\n",
        "        k = bool(full_file_hash)\n",
        "        if k is True:\n",
        "            length = len(full_file_hash)\n",
        "            x = \"file\" if length == 1  else \"files\"\n",
        "            print(\"\\n\" + str(length) + \" duplicate \" + x + \" found.\")\n",
        "            print(\"\\n[x] Duplicates files [x]\")\n",
        "        else:\n",
        "            print('\\nNo duplicate files found.')\n",
        "\n",
        "        for k,v in full_file_hash.items():\n",
        "            print(\"\\nMD5 hash of the duplicate files: \" + k + \"\\n\")\n",
        "            for files in v:\n",
        "                print(files)\n",
        "    else:\n",
        "        print(\"\\nNo duplicate files found.\")\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBAQGuUy48Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6B8zfVs48IN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ca9da813-c129-4618-9e16-ad98b98fd19a"
      },
      "source": [
        "import os\n",
        "\n",
        "lines_seen = set() # holds lines already seen\n",
        "outfile = open('out.txt', \"w\")\n",
        "path = r'/content/a'\n",
        "for file in os.listdir(path): #added this line\n",
        "    current_file = os.path.join(path, file)\n",
        "    for line in open(current_file, \"r\"):\n",
        "        if line not in lines_seen: # not a duplicate\n",
        "            outfile.write(line)\n",
        "            lines_seen.add(line)\n",
        "outfile.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fc3a2b0af384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#added this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcurrent_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines_seen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# not a duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/a/.ipynb_checkpoints'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB630quHuCZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}